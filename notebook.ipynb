{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e38d4bc5-fee2-0f42-feb3-c2716bfeabdf",
    "_uuid": "599d9ca08cb26c1f68a74eb9bfb6b9f7e9e945b8"
   },
   "source": [
    "This is a notebook to play around with TensorFlow and convolutional neural networks.\n",
    "\n",
    "We're using a convnet with the following structure (like VGGNet but smaller):\n",
    "\n",
    "    input    28x28x1\n",
    "    conv1_1  32 kernels\n",
    "    conv1_2  32 kernels\n",
    "    pool1    14x14x32\n",
    "    conv2_1  64 kernels\n",
    "    conv2_2  64 kernels\n",
    "    pool2    7x7x64\n",
    "    fc3      7x7x64x128   with dropout\n",
    "    fc4      128          with dropout\n",
    "    fc5      10           softmax\n",
    "    \n",
    "All convolutions are 3x3. All pooling is 2x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "108c8c49-88a7-8c55-44e9-7608a8f90f8c",
    "_uuid": "0232eb4ba9f2a88991a670ad2a01d79e4fe72b11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.figsize'] = (16.0, 8.0)   # change default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7fa35589-87ac-cd0f-d49e-4523ed9467ab",
    "_uuid": "1405436e828d5e305019af00af0b23138660b1da"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels.\n",
    "\n",
    "The pixels are values between 0 and 255. We convert those to floats in the range 0.0 - 1.0.\n",
    "\n",
    "The training data has 42000 rows. Each row in the training data CSV begins with the label, followed by the 784 pixel values.\n",
    "\n",
    "The test set has 28000 rows, but we don't have the labels. For evaluating the network we'll make our own test set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ab191242-96cf-4f0b-e7c9-ffb800b5e5a2",
    "_uuid": "11042fb95047e656a54753069903363bd8d58af5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv as csv\n",
    "\n",
    "image_width = 28\n",
    "image_height = 28\n",
    "num_pixels = image_width * image_height\n",
    "\n",
    "# Unfortunately, using the entire data set exceeds Kaggle's memory limit.\n",
    "#num_examples = 42000\n",
    "num_examples = 2000\n",
    "\n",
    "data_X = np.zeros((num_examples, num_pixels))\n",
    "data_y = np.zeros(num_examples, dtype=np.int)\n",
    "\n",
    "with open(\"../input/train.csv\", \"rt\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    \n",
    "    for j, row in enumerate(reader):\n",
    "        if j == num_examples:\n",
    "            break\n",
    "        for (i, col) in enumerate(row):\n",
    "            if i == 0:\n",
    "                data_y[j] = int(col)\n",
    "            else:\n",
    "                data_X[j][i - 1] = float(col) / 255\n",
    "                \n",
    "print(\"data_X is %d bytes\" % data_X.nbytes)\n",
    "print(\"data_y is %d bytes\" % data_y.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b69170fd-1e70-e8b8-e01d-06f6e074bf85",
    "_uuid": "fea043c192c665e83d43032c27265ddaa43be1ee"
   },
   "source": [
    "Show the pixel data for a few images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9d79fdfe-ef43-fd50-4350-085d578fb49f",
    "_uuid": "0752e850384abf7c46bf88bce0c8167b4149a7b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "num_horz = 4\n",
    "num_vert = 4\n",
    "for i in range(num_horz * num_vert):\n",
    "    ax = fig.add_subplot(num_vert, num_horz, i + 1, xticks=[], yticks=[])\n",
    "    image_data = (data_X[i] * 255).reshape(image_width, image_height)\n",
    "    ax.imshow(image_data, cmap=cm.Greys_r, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c13a8ed-459b-bd78-da43-885f559a84f9",
    "_uuid": "476d1035de75a7af3dc296f808a35c9b5ebd61a1"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The labels must be one-hot encoded, i.e. for each input example there is a 10-element vector with all zeros and a single 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1354940-5576-d6de-2a10-b40d79586035",
    "_uuid": "1a91dd373e119375fe9fe0e7cf486b2baa9071e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(labels, num_outputs):\n",
    "    m = labels.shape[0]\n",
    "    y = np.zeros((m, num_outputs))\n",
    "    for i, label in enumerate(labels):\n",
    "        y[i][int(label)] = 1.0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60edd0d7-1378-7804-4e3d-1704605c819e",
    "_uuid": "3035d8711a56e72c84eac43cc10769093a19518d"
   },
   "source": [
    "Randomly split the data into train, validate, and test sets. Ideally, each class is represented in equal amounts in the three sets.\n",
    "\n",
    "For speed reasons, we're not doing cross validation but will only use a single set of validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83da47a3-601d-6d9c-2f51-936c04d2acaf",
    "_uuid": "6ebf63423a1ff65a6eaeb64100c81408d618a02b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "indices = np.random.permutation(len(data_X))\n",
    "\n",
    "num_train = int(0.6 * num_examples)\n",
    "num_val = int(0.2 * num_examples)\n",
    "num_test = num_examples - num_val - num_train\n",
    "\n",
    "X_train = data_X[indices[:num_train]]\n",
    "y_train = data_y[indices[:num_train]]\n",
    "X_val   = data_X[indices[num_train:-num_test]]\n",
    "y_val   = data_y[indices[num_train:-num_test]]\n",
    "X_test  = data_X[indices[-num_test:]]\n",
    "y_test  = data_y[indices[-num_test:]]\n",
    "\n",
    "X_mean = np.zeros((1, num_pixels))\n",
    "X_std = np.zeros((1, num_pixels))\n",
    "\n",
    "y_train_labels = y_train\n",
    "y_val_labels   = y_val\n",
    "y_test_labels  = y_test\n",
    "\n",
    "y_train = onehot(y_train, 10)\n",
    "y_val   = onehot(y_val, 10)\n",
    "y_test  = onehot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aab585f0-1a53-99ad-bb86-72fede35b6e5",
    "_uuid": "a502748823d0f1ff6b6640988e091c0987789c2c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%xdel data_X\n",
    "%xdel data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dcdb6997-7b2f-7667-489a-20f043d33b8b",
    "_uuid": "97baa07e7935878325d26a93dfaa305afb38a275"
   },
   "source": [
    "Normalize the input data. Calculate the mean and standard deviation of each feature on the training data, not on validation or test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "67060559-e994-4ab9-6eec-1ed241f309e1",
    "_uuid": "230fa3a16d15bc6fb9b5a1f2f1829a1de637bd65",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mean = np.mean(X_train, axis=0, keepdims=True)\n",
    "X_std = np.std(X_train, axis=0, keepdims=True)\n",
    "\n",
    "X_train -= X_mean\n",
    "X_val   -= X_mean\n",
    "X_test  -= X_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "442ad61f-a15b-722c-deaf-e581ff5a89e3",
    "_uuid": "c7e32ecc9f5bd9b57a20444d5abb30e2d0c3b122"
   },
   "source": [
    "Plot the mean image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4900e1a9-2826-2cf8-c64d-71a47cabeddd",
    "_uuid": "0e943c0c8846c8ab12a78061994ed5b4614fe7ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[])\n",
    "ax.set_frame_on(False)\n",
    "ax.set_axis_off()\n",
    "image_data = (X_mean * 255).reshape(image_width, image_height)\n",
    "ax.imshow(image_data, cmap=cm.Greys_r, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ce79058-a614-9cd5-e895-6df0d4740b96",
    "_uuid": "b5ad1840107311dc4502be9f4f6b640525bcc95f"
   },
   "source": [
    "## Network architecture\n",
    "\n",
    "Create the model, a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0dea2c3f-a1b4-53fe-b54b-f92a1a08013f",
    "_uuid": "1aae1b6891de5348acf9b91edc10742686164abc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "tf.set_random_seed(7777)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the weights with a small amount of noise.\n",
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())    \n",
    "\n",
    "# Use a slightly positive initial bias to avoid dead ReLUs.\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution with stride 1 and enough zero padding to keep width/height the same.\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Plain old max pooling over 2x2 blocks.\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# The input data.\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")\n",
    "y = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "\n",
    "with tf.name_scope(\"hyperparameters\"):\n",
    "    learning_rate = tf.placeholder(tf.float32, name=\"learning-rate\")\n",
    "    reg_lambda = tf.placeholder(tf.float32, name=\"regularization\")\n",
    "\n",
    "# Reshape x into a 4d tensor. -1 is because we don't know the number of examples\n",
    "# yet. 28x28 is the width and height, 1 is the number of color channels.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "with tf.name_scope(\"conv1_1\"):\n",
    "    W_conv1_1 = weight_variable(\"W_conv1_1\", [3, 3, 1, 32])\n",
    "    b_conv1_1 = bias_variable([32])\n",
    "    h_conv1_1 = tf.nn.relu(conv2d(x_image, W_conv1_1) + b_conv1_1)\n",
    "\n",
    "with tf.name_scope(\"conv1_2\"):\n",
    "    W_conv1_2 = weight_variable(\"W_conv1_2\", [3, 3, 32, 32])\n",
    "    b_conv1_2 = bias_variable([32])\n",
    "    h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, W_conv1_2) + b_conv1_2)\n",
    "\n",
    "with tf.name_scope(\"pool1\"):\n",
    "    h_pool1 = max_pool_2x2(h_conv1_2)\n",
    "\n",
    "with tf.name_scope(\"conv2_1\"):\n",
    "    W_conv2_1 = weight_variable(\"W_conv2_1\", [3, 3, 32, 64])\n",
    "    b_conv2_1 = bias_variable([64])\n",
    "    h_conv2_1 = tf.nn.relu(conv2d(h_pool1, W_conv2_1) + b_conv2_1)\n",
    "\n",
    "with tf.name_scope(\"conv2_2\"):\n",
    "    W_conv2_2 = weight_variable(\"W_conv2_2\", [3, 3, 64, 64])\n",
    "    b_conv2_2 = bias_variable([64])\n",
    "    h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, W_conv2_2) + b_conv2_2)\n",
    "\n",
    "with tf.name_scope(\"pool2\"):\n",
    "    h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "    # Reshape the output into a vector that we can pass to the FC layer.\n",
    "    h_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "with tf.name_scope(\"fc3\"):\n",
    "    W_fc3 = weight_variable(\"W_fc3\", [7 * 7 * 64, 128])\n",
    "    b_fc3 = bias_variable([128])\n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_flat, W_fc3) + b_fc3)\n",
    "\n",
    "with tf.name_scope(\"fc3-dropout\"):\n",
    "    # Apply dropout to reduce overfitting.\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"dropout-probability\")\n",
    "    h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"fc4\"):\n",
    "    W_fc4 = weight_variable(\"W_fc4\", [128, 128])\n",
    "    b_fc4 = bias_variable([128])\n",
    "    h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "with tf.name_scope(\"fc4-dropout\"):\n",
    "    h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"fc5\"):\n",
    "    W_fc5 = weight_variable(\"W_fc5\", [128, 10])\n",
    "    b_fc5 = bias_variable([10])\n",
    "    y_pred = tf.nn.softmax(tf.matmul(h_fc4_drop, W_fc5) + b_fc5)\n",
    "\n",
    "# Softmax, so use cross entropy loss.\n",
    "with tf.name_scope(\"loss-function\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    \n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(W_fc3) + tf.nn.l2_loss(b_fc3) +\n",
    "                    tf.nn.l2_loss(W_fc4) + tf.nn.l2_loss(b_fc4) +\n",
    "                    tf.nn.l2_loss(W_fc5) + tf.nn.l2_loss(b_fc5))\n",
    "    loss += 5e-4 * regularizers\n",
    "\n",
    "# Use ADAM (instead of plain gradient descent).\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "# The accuracy op computes the % correct on a dataset with labels. \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# For doing inference on the test set without labels.\n",
    "with tf.name_scope(\"inference\"):\n",
    "    inference = tf.argmax(y_pred, 1)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c4ac49b7-a667-addd-e459-e5a73808414d",
    "_uuid": "9cee7f06c4d3cd73e577cc688b8072f47edc0f37"
   },
   "source": [
    "## Training\n",
    "\n",
    "We train in mini batches. The `next_batch()` function returns the next mini batch of training data. If we reach the end of the training set, it counts as an epoch. We then shuffle the data and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6485bfbf-4bee-c4b8-077e-51ff2b649447",
    "_uuid": "4ad980a536daae85ed54fb0a506d043cc88ed168",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_in_epoch = 0\n",
    "epochs_completed = 0\n",
    "\n",
    "# Based on code from learn/datasets/mnist.py\n",
    "def next_batch(batch_size):\n",
    "    global index_in_epoch, epochs_completed, X_train, y_train, y_train_labels\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # Epoch completed?\n",
    "    if index_in_epoch > len(X_train):\n",
    "        # Shuffle the data.\n",
    "        perm = np.arange(len(X_train))\n",
    "        np.random.shuffle(perm)\n",
    "        X_train = X_train[perm]\n",
    "        y_train = y_train[perm]\n",
    "        y_train_labels = y_train_labels[perm]\n",
    "\n",
    "        # Start next epoch.\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        epochs_completed += 1\n",
    "\n",
    "    end = index_in_epoch\n",
    "    return X_train[start:end], y_train[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90b99508-0bb3-6422-22e1-6b457044df0c",
    "_uuid": "e184bf70b456bcc4906366c52b033996f62c8f06"
   },
   "source": [
    "Helper function that is used by grid search and random search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff290e6a-8244-8e89-ab3f-48baaa07446b",
    "_uuid": "a1e07b0866252fcd7415cc434369cfd3d1385f54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def train_and_validate(max_steps, batch_size, print_every, verbose=True, acceptable_loss=0.001, smooth_loss=False):\n",
    "    sess.run(init)\n",
    "\n",
    "    loss_history = []\n",
    "    loss_avg = 0\n",
    "    smooth_steps = 20\n",
    "\n",
    "    # Used by next_batch()\n",
    "    global index_in_epoch, epochs_completed\n",
    "    index_in_epoch = 0\n",
    "    epochs_completed = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get the next mini-batch of training data.\n",
    "        batch_xs, batch_ys = next_batch(100)\n",
    "        feed_dict = { x: batch_xs, y: batch_ys, learning_rate: lr, reg_lambda: reg, keep_prob: 0.5 }\n",
    "\n",
    "        # Run the network.\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # If enabled, we calculate the average loss over the last X steps, since\n",
    "        # the loss can be a bit jittery when using stochastic gradient descent.\n",
    "        if smooth_loss:\n",
    "            loss_avg += loss_value\n",
    "            if step % smooth_steps == 0:\n",
    "                if step > 0: loss_avg /= smooth_steps\n",
    "                loss_history.append(loss_avg)\n",
    "                loss_avg = 0\n",
    "        else:\n",
    "            loss_history.append(loss_value)\n",
    "        \n",
    "        # Print the loss once every so many steps.\n",
    "        if (step % print_every == 0) and verbose:\n",
    "            print(\"    step: %4d, epoch: %2d, loss: %.3f (%.3f sec)\" % \\\n",
    "                      (step, epochs_completed, loss_value, duration))\n",
    "\n",
    "        # Stop the gradient descent if a user-specified loss is reached.\n",
    "        if loss_value <= acceptable_loss or math.isnan(loss_value):\n",
    "            print(\"    Loss is below acceptable limit, ending training after %d steps\" % step)\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Final loss: %f\" % loss_value)\n",
    "\n",
    "    # Calculate cross validation score.\n",
    "    score = sess.run(accuracy, feed_dict={x: X_val, y: y_val, keep_prob: 1.0})\n",
    "    if verbose:\n",
    "        print(\"Validation score: %g\" % score)\n",
    "\n",
    "    return loss_history, loss_value, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b74415e-a4d0-90f9-1ca7-9546b7220bbc",
    "_uuid": "90a5c1f326a6ae800baed170d645e491766d567a"
   },
   "source": [
    "To do a grid search to find the best learning rate and regularization, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1100d49f-59f5-b97e-aa61-a2da73042c0f",
    "_uuid": "7ddb193e56bd5bef8bbc9edc5467b141106a4027",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters used in the grid search\n",
    "learning_rates = [0.001, 0.003, 0.01]\n",
    "reg_lambdas = [0, 0.1, 0.3]            # regularization strength\n",
    "\n",
    "from itertools import product\n",
    "grid = list(product(learning_rates, reg_lambdas))\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b46aa295-0e64-18b5-6a5f-156aa2e7542b",
    "_uuid": "49fd6788c7a5252b17deb865d71fc389eb1b01ca"
   },
   "source": [
    "To do a random search, run the next cell. \n",
    "\n",
    "This gives better results than grid search. Because we're doing many random searches, you want to limit the number of epochs or it takes forever. First you run a coarse search, then you run a finer search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16658ad7-a3c8-2aed-6ff1-bc9c983d6457",
    "_uuid": "b5fbd0597959c2e9b21ae52d90cb19bd0a6cd0b5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many random searches to perform\n",
    "max_search = 10\n",
    "\n",
    "grid = []\n",
    "for i in range(max_search):\n",
    "    lr = 10**np.random.uniform(-5, -2)\n",
    "    reg = 10**np.random.uniform(-3, 1)\n",
    "    grid.append((lr, reg))\n",
    "    \n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de35c96b-35c6-de03-d94f-c3d0f517f843",
    "_uuid": "d1a30530dffaee110129d732263a5ae4be2087b7"
   },
   "source": [
    "To do final training before you do inference, run the next cell. This uses the learning rate and regularization that you found with grid search or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "323b25b1-9371-408b-692c-f93e90a3960d",
    "_uuid": "adda18ab2527c7f9da9d3770df454492a0dc5c6e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = [(0.0003, 0.0)]\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc99fa0f-8a16-8407-bc10-f584ef10a6b5",
    "_uuid": "a0e52e7512fb92313a26397894159ed04c8cf65c"
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d6e09c2-0c7b-d20b-5224-d413ee90a108",
    "_uuid": "b22c3faf5c6d7f285e0b4a9382df558c4bfed4f2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training %d examples\" % X_train.shape[0])\n",
    "\n",
    "scores = []\n",
    "start_time = time.time()\n",
    "loss_history = {}\n",
    "\n",
    "for i, params in enumerate(grid):\n",
    "    lr, reg = params\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"*** learning rate: %g, regularization: %g\" % (lr, reg))\n",
    "    \n",
    "    hist, loss_value, score = train_and_validate(max_steps=500, batch_size=100, print_every=50,\\\n",
    "                                                 verbose=verbose, acceptable_loss=0.001, smooth_loss=False)\n",
    "\n",
    "    if not verbose:\n",
    "        print(\"score: %0.6f, loss: %0.6f, rate: %0.6f, reg: %0.6f (%d/%d)\" % (score, loss_value, lr, reg, i+1, max_search))\n",
    "\n",
    "    key = \"learn: %g, reg: %g\" % (lr, reg)\n",
    "    loss_history[key] = hist\n",
    "    scores.append(score)\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "\n",
    "print(\"Best validation score: %g\" % np.max(scores))\n",
    "print(\"Best parameters:\", grid[np.argmax(scores)])\n",
    "print(\"Time: %f sec\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c51d4b6-a692-a733-e184-3e032c480d40",
    "_uuid": "81dfda83ab35b41cd5be6e2c6aaac1ff5bc62c2e"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Using the trained network to predict results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a709d07-b529-f414-550b-32a3da4a7f67",
    "_uuid": "97174894f25a64d6759872065150fb1bc8ef8807",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Calculate accuracy on training data. This should be a good score, \n",
    "# but not *too* good or we're overfitting.\n",
    "print(sess.run(accuracy, feed_dict={x: X_train, y: y_train, keep_prob: 1.0}))\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(\"duration %f sec\" % duration)\n",
    "\n",
    "# Calculate accuracy on validation data.\n",
    "print(sess.run(accuracy, feed_dict={x: X_val, y: y_val, keep_prob: 1.0}))\n",
    "\n",
    "# Calculate accuracy on test data. We do have labels for these examples, \n",
    "# but the examples themselves were not used to train the network.\n",
    "print(sess.run(accuracy, feed_dict={x: X_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dd71f751-5c7c-9ea9-fcaf-61e75411fab2",
    "_uuid": "b5d52f6bc88959a1d086a89cb6c33c5fdba1a540"
   },
   "source": [
    "## Confusion matrix\n",
    "\n",
    "This shows how often any two classes are mixed up by the prediction. `C[i, j]` is the number of observations known to be in group `i` but predicted to be in group `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80d4215f-cc3c-be22-f3ee-792e5c804b89",
    "_uuid": "82366af365dedaf84681628581d718593b5a5cbd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(target, predicted):\n",
    "    assert(target.shape == predicted.shape)\n",
    "    \n",
    "    num_classes = len(np.unique(target))\n",
    "    confusion = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    for i in range(len(target)):      \n",
    "        confusion[target[i], predicted[i]] += 1\n",
    "    \n",
    "    return confusion\n",
    "\n",
    "def plot_confusion_matrix(conf):\n",
    "    plt.imshow(conf, interpolation='nearest', cmap=plt.cm.binary)\n",
    "    plt.xticks(range(conf.shape[1]))\n",
    "    plt.yticks(range(conf.shape[0]))\n",
    "    plt.xlabel(\"predicted label\")\n",
    "    plt.ylabel(\"true label\")\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "    \n",
    "pred = sess.run(inference, feed_dict={x: X_test, keep_prob: 1.0})\n",
    "\n",
    "conf = confusion_matrix(y_test_labels, pred)\n",
    "print(conf)\n",
    "\n",
    "plot_confusion_matrix(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "170d7d6c-cdbb-eb7a-9477-94db5054885f",
    "_uuid": "ef73974105690797093ede7db189b27920f5c51a"
   },
   "source": [
    "### Plot of the loss\n",
    "\n",
    "By showing how the loss changes with every training epoch, we can see if the learning rate is any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2713cc70-13c7-18cd-1c05-cd46ff5fc6ec",
    "_uuid": "133e463ae8869633539e1e8fa42b47b6c172e842",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in loss_history.items():\n",
    "    plt.plot(v, label=k)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2cc45da-8e93-c3c1-4914-dc5abfdb0341",
    "_uuid": "23f2e17e1c4aed9bfda2a4c34379e1fc077f4494"
   },
   "source": [
    "## Kaggle competition test set\n",
    "\n",
    "Just for fun, use the trained model to make predictions on the test set from the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bb7a53f4-1b3c-ff80-b993-bba48b67cc20",
    "_uuid": "aef5a2640c5da8b4809a2fc43e0aee888b17c3aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Free up memory we don't need anymore.\n",
    "%xdel X_train\n",
    "%xdel X_test\n",
    "%xdel X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22d1da9b-d908-7cc7-64e6-b15226269338",
    "_uuid": "29a8e7046138d207c2041e5cbd8eafe1560d0e86",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change this to \"if True\" if you want to run this part of the script.\n",
    "if False:\n",
    "    import csv as csv\n",
    "\n",
    "    num_test_examples = 28000\n",
    "    X_kaggle = np.zeros((num_test_examples, num_pixels))\n",
    "\n",
    "    with open(\"../input/test.csv\", \"rt\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "\n",
    "        for j, row in enumerate(reader):\n",
    "            for (i, col) in enumerate(row):\n",
    "                X_kaggle[j][i - 1] = float(col) / 255\n",
    "\n",
    "    X_kaggle -= X_mean\n",
    "\n",
    "    print(\"X_kaggle is %d bytes\" % X_kaggle.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e55ea9fa-50cf-c9e6-5ed6-7f62112332b0",
    "_uuid": "12c51fc3f6e67fc272c4dc3099589bd68414e03f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    pred = sess.run(inference, feed_dict={x: X_kaggle, keep_prob: 1.0})\n",
    "    print(pred)\n",
    "    \n",
    "    with open(\"prediction.txt\", \"wt\") as f:\n",
    "        f.write('\"ImageId\",\"Label\"\\n')\n",
    "        for i in range(num_test_examples):\n",
    "            f.write('%d,\"%d\"\\n' % (i+1, pred[i]))    "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
